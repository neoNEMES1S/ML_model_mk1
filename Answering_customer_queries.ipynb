{"cells": [{"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Use Watsonx to respond to natural language questions using RAG approach"}, {"metadata": {}, "cell_type": "raw", "source": "# **Note:** Please note that for the watsonx challenge, please consider running these notebooks locally on your laptop/desktop."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n\nSome familiarity with Python is helpful. This notebook uses Python 3.10.\n\n#### About Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n\nIn its simplest form, RAG requires 3 steps:\n\n- Index knowledge base passages (once)\n- Retrieve relevant passage(s) from knowledge base (for every user query)\n- Generate a response by feeding retrieved passage into a large language model (for every user query)\n"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n##  Set up the environment\n\n"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Install and import dependecies\n\n**Note:** For Windows environments, please remove `| tail -n 1` commands in the cell below."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "!pip install chromadb==0.3.27 | tail -n 1\n!pip install sentence_transformers | tail -n 1\n!pip install pandas | tail -n 1\n!pip install rouge_score | tail -n 1\n!pip install nltk | tail -n 1\n!pip install \"ibm-watson-machine-learning>=1.0.312\" | tail -n 1", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Successfully installed anyio-3.7.1 backoff-2.2.1 chromadb-0.3.27 clickhouse-connect-0.6.8 coloredlogs-15.0.1 duckdb-0.8.1 exceptiongroup-1.1.3 fastapi-0.85.1 h11-0.14.0 hnswlib-0.7.0 httptools-0.6.0 humanfriendly-10.0 lz4-4.3.2 monotonic-1.6 mpmath-1.3.0 onnxruntime-1.15.1 overrides-7.4.0 posthog-3.0.1 pulsar-client-3.2.0 pydantic-1.9.0 python-dotenv-1.0.0 sniffio-1.3.0 starlette-0.20.4 sympy-1.12 tokenizers-0.13.3 tqdm-4.66.1 typing-extensions-4.7.1 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 zstandard-0.21.0\nSuccessfully installed filelock-3.12.2 huggingface-hub-0.16.4 nltk-3.8.1 regex-2023.8.8 safetensors-0.3.2 sentence_transformers-2.2.2 transformers-4.31.0\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nSuccessfully installed rouge_score-0.1.2\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk) (2023.8.8)\nSuccessfully installed ibm-watson-machine-learning-1.0.316\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** Please restart the notebook kernel to pick up proper version of packages installed above."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "import os, getpass\nimport pandas as pd\nfrom typing import Optional, Dict, Any, Iterable, List\n\ntry:\n    from sentence_transformers import SentenceTransformer\nexcept ImportError:\n    raise ImportError(\"Could not import sentence_transformers: Please install sentence-transformers package.\")\n    \ntry:\n    import chromadb\n    from chromadb.api.types import EmbeddingFunction\nexcept ImportError:\n    raise ImportError(\"Could not import chromdb: Please install chromadb package.\")", "execution_count": 2, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud user API key. For details, see\n[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}", "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Please enter your WML api key (hit enter): \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the project id\nThe API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n\n**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "try:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": 4, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"data\"></a>\n## Train/test data loading"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Load train and test datasets. At first, training dataset (`train_data`) should be used to work with the models to prepare and tune prompt. Then, test dataset (`test_data`) should be used to calculate the metrics score for selected model, defined prompts and parameters."}, {"metadata": {}, "cell_type": "code", "source": "data_file_location = \"watsonx_challenge_evaluation_notebooks.zip\"\nknowledge_base_location = \"data/knowledge_base.zip\"\nif not os.path.exists(data_file_location) or not os.path.exists(knowledge_base_location):\n    import requests\n    req = requests.get('https://watsonxai-data-sets.s3.us-south.cloud-object-storage.appdomain.cloud/watsonx_challenge_evaluation_notebooks.zip')\n    open(data_file_location , 'wb').write(req.content)\n    from zipfile import ZipFile\n    with ZipFile(data_file_location, 'r') as zObject:\n        zObject.extractall(\".\")\n    req = requests.get('https://watsonxai-data-sets.s3.us-south.cloud-object-storage.appdomain.cloud/knowledge_base.zip')\n    open(knowledge_base_location , 'wb').write(req.content)\n\nfilename_test = 'data/RAG/nq910_400_instances/test.tsv'\nfilename_train = 'data/RAG/nq910_400_instances/train.tsv'\n\ntest_data = pd.read_csv(filename_test, delimiter='\\t')\ntrain_data = pd.read_csv(filename_train, delimiter='\\t')", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_data.head()", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "    qid                                           question relevant  \\\n0  8519  when was the cathedral of santa maria del fior...      136   \n1   852             who plays cassidy on law and order svu      177   \n2  7600      who was the old woman in phantom of the opera      642   \n3  4258     when is the finals of americas got talent 2017      781   \n4  8272           who played the frog in gnomeo and juliet     1018   \n\n                            answers  \n0  begun in 1296::completed by 1436  \n1                      Dean Winters  \n2                       Madame Giry  \n3               September 20 , 2017  \n4                     Ashley Jensen  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question</th>\n      <th>relevant</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8519</td>\n      <td>when was the cathedral of santa maria del fior...</td>\n      <td>136</td>\n      <td>begun in 1296::completed by 1436</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>852</td>\n      <td>who plays cassidy on law and order svu</td>\n      <td>177</td>\n      <td>Dean Winters</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7600</td>\n      <td>who was the old woman in phantom of the opera</td>\n      <td>642</td>\n      <td>Madame Giry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4258</td>\n      <td>when is the finals of americas got talent 2017</td>\n      <td>781</td>\n      <td>September 20 , 2017</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8272</td>\n      <td>who played the frog in gnomeo and juliet</td>\n      <td>1018</td>\n      <td>Ashley Jensen</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "test_data.head()", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "    qid                                          question  relevant  \\\n0  5555           who did chris carter play for last year       267   \n1  6654       what is the latest version of safari on mac       664   \n2  3396  when did bucharest become the capital of romania       944   \n3  8198  who did jeffrey dean morgan play on supernatural      1398   \n4  4526           who is the shortest man that ever lived      1522   \n\n                 answers  \n0      Milwaukee Brewers  \n1              Safari 11  \n2                   1862  \n3   John Eric Winchester  \n4  Chandra Bahadur Dangi  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question</th>\n      <th>relevant</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5555</td>\n      <td>who did chris carter play for last year</td>\n      <td>267</td>\n      <td>Milwaukee Brewers</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6654</td>\n      <td>what is the latest version of safari on mac</td>\n      <td>664</td>\n      <td>Safari 11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3396</td>\n      <td>when did bucharest become the capital of romania</td>\n      <td>944</td>\n      <td>1862</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8198</td>\n      <td>who did jeffrey dean morgan play on supernatural</td>\n      <td>1398</td>\n      <td>John Eric Winchester</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4526</td>\n      <td>who is the shortest man that ever lived</td>\n      <td>1522</td>\n      <td>Chandra Bahadur Dangi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "## Build up knowledge base\n\nThe current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n\nWe can generate dense vector representations using embedding models. In this notebook, we use [SentenceTransformers](https://www.google.com/search?client=safari&rls=en&q=sentencetransformers&ie=UTF-8&oe=UTF-8) [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to embed both the knowledge base passages and user queries. `all-MiniLM-L6-v2` is a performant open-source model that is small enough to run locally.\n\nA vector database is optimized for dense vector indexing and retrieval. This notebook uses [Chroma](https://docs.trychroma.com), a user-friendly open-source vector database, licensed under Apache 2.0, which offers good speed and performance with all-MiniLM-L6-v2 embedding model."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "The dataset we are using is already split into self-contained passages that can be ingested by Chroma. \n\nThe size of each passage is limited by the embedding model's context window (which is 256 tokens for `all-MiniLM-L6-v2`)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Load knowledge base documents\n\nLoad set of documents used further to build knowledge base. "}, {"metadata": {}, "cell_type": "code", "source": "data_root = \"data\"\nknowledge_base_dir = f\"{data_root}/knowledge_base\"", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "if not os.path.exists(knowledge_base_dir):\n    from zipfile import ZipFile\n    with ZipFile(knowledge_base_dir + \".zip\", 'r') as zObject:\n        zObject.extractall(data_root)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "documents = pd.read_csv(f\"{knowledge_base_dir}/psgs.tsv\", sep='\\t', header=0)\ndocuments['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']", "execution_count": 11, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Create an embedding function\n\nNote that you can feed a custom embedding function to be used by chromadb. The performance of chromadb may differ depending on the embedding model used."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n    def __call__(self, texts):\n        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\nemb_func = MiniLML6V2EmbeddingFunction()", "execution_count": 12, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Set up Chroma upsert\n\nUpserting a document means update the document even if it exists in the database. Otherwise re-inserting a document throws an error. This is useful for experimentation purpose."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "class ChromaWithUpsert:\n    def __init__(\n            self,\n            name: Optional[str] = \"watsonx_rag_collection\",\n            persist_directory:Optional[str]=None,\n            embedding_function: Optional[EmbeddingFunction]=None,\n            collection_metadata: Optional[Dict] = None,\n    ):\n        self._client_settings = chromadb.config.Settings()\n        if persist_directory is not None:\n            self._client_settings = chromadb.config.Settings(\n                chroma_db_impl=\"duckdb+parquet\",\n                persist_directory=persist_directory,\n            )\n        self._client = chromadb.Client(self._client_settings)\n        self._embedding_function = embedding_function\n        self._persist_directory = persist_directory\n        self._name = name\n        self._collection = self._client.get_or_create_collection(\n            name=self._name,\n            embedding_function=self._embedding_function\n            if self._embedding_function is not None\n            else None,\n            metadata=collection_metadata,\n        )\n\n    def upsert_texts(\n        self,\n        texts: Iterable[str],\n        metadata: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n        Args:\n            :param texts (Iterable[str]): Texts to add to the vectorstore.\n            :param metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            :param ids (Optional[List[str]], optional): Optional list of IDs.\n            :param metadata: Optional[List[dict]] - optional metadata (such as title, etc.)\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        # TODO: Handle the case where the user doesn't provide ids on the Collection\n        if ids is None:\n            import uuid\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n        self._collection.upsert(\n            metadatas=metadata, documents=texts, ids=ids\n        )\n        return ids\n\n    def is_empty(self):\n        return self._collection.count()==0\n\n    def persist(self):\n        self._client.persist()\n\n    def query(self, query_texts:str, n_results:int=5):\n        \"\"\"\n        Returns the closests vector to the question vector\n        :param query_texts: the question\n        :param n_results: number of results to generate\n        :return: the closest result to the given question\n        \"\"\"\n        return self._collection.query(query_texts=query_texts, n_results=n_results)", "execution_count": 13, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Embed and index documents with Chroma\n\n**Note: Could take several minutes if you don't have pre-built indices**"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "%%time\nchroma = ChromaWithUpsert(\n    name=f\"nq910_minilm6v2\",\n    embedding_function=emb_func,  # you can have something here using /embed endpoint\n    persist_directory=knowledge_base_dir,\n)\nif chroma.is_empty():\n    _ = chroma.upsert_texts(\n        texts=documents.indextext.tolist(),\n        # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n        metadata=[{'title': title, 'id': id}\n                  for (title,id) in\n                  zip(documents.title, documents.id)],  # filter on these!\n        ids=[str(i) for i in documents.id],  # unique for each doc\n    )\n    chroma.persist()", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "CPU times: user 635 ms, sys: 249 ms, total: 884 ms\nWall time: 896 ms\n", "name": "stdout"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"models\"></a>\n## Foundation Models on Watsonx"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "You need to specify `model_id` that will be used for inferencing."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "**Action**: Use `FLAN_UL2` model."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes", "execution_count": 15, "outputs": []}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "model_id = ModelTypes.FLAN_UL2", "execution_count": 16, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"predict\"></a>\n## Generate a retrieval-augmented response to a question"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Select questions\n\nGet questions from the previously loaded test dataset."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "question_texts = [q.strip(\"?\") + \"?\" for q in test_data['question'].tolist()]\nprint(\"\\n\".join(question_texts))", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "who did chris carter play for last year?\nwhat is the latest version of safari on mac?\nwhen did bucharest become the capital of romania?\nwho did jeffrey dean morgan play on supernatural?\nwho is the shortest man that ever lived?\nhow many seconds do you have to throw a grenade?\nwho is known as a father of indian cricket?\nwhat is the name of the period in japanese history that began in 1868?\nharold and kumar go to white castle where was it filmed?\nhow many games have the capitals won in the playoffs?\nwhat is the best selling nintendo game of all time?\nkorean movie about a man on an island?\nwho plays lorelai in hannah montana the movie?\nwhat season of greys anatomy was the plane crash?\nwho made call of duty black ops 2?\nwhen was the last solar eclipse seen in north america?\nwho is hosting the fifa world cup in 2022?\nwhat is the record for wins in major league baseball?\nbig boss 2 telugu set location in hyderabad?\nwho did the us support in the korean war?\nwhat is the caterpillar smoking in alice in wonderland?\nhow much aid does us give to other countries?\nwho plays jake on two and a half?\nwhat do you call a bundle of hay?\nhow many levels are there in science olympiad?\nwhere are the singers of florida georgia line from?\nwho sings lead on wouldnt it be nice?\nwho is the main character in shadow of mordor?\nwhere are the next olympics to be held?\nwhen did game of thrones season 7 start?\nwhen does hook show up in once upon a time?\nwhen was the last time georgia tech won a national championship?\nwho plays victor in days of our lives?\nwho won the last triple crown in baseball?\nwhen does the football transfer window open in 2018?\nwhat episode of mr young do adam and echo kiss?\nwhen is game of thrones season 7 episode 7 releasing?\nwho is the architect of sheikh zayed mosque?\nwho is the director of iron man 3?\nwho is one of the first german composers that we know about?\nwhere do they move to in cheaper by the dozen?\nwho played bass on and justice for all?\nzen and the art of motorcycle maintenance bikes?\nwhen did texas become part of united states?\nthe atlantoaxial joint is an example of what type of joint?\nwhat is the fastest roller coaster in california?\nwhen did they start to build the great wall of china?\nwhat type of reaction is the rusting of iron?\nwhere do the flyers play their home games?\nwhere does something old something new something borrowed something blue come from?\n", "name": "stdout"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Retrieve relevant context\n\nFetch paragraphs similar to the question."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "relevant_contexts = []\n\nfor question_text in question_texts:\n    relevant_chunks = chroma.query(\n        query_texts=[question_text],\n        n_results=5,\n    )\n    relevant_contexts.append(relevant_chunks)", "execution_count": 18, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Get the set of chunks for one of the questions."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "sample_chunks = relevant_contexts[0]\nfor i, chunk in enumerate(sample_chunks['documents'][0]):\n    print(\"=========\")\n    print(\"Paragraph index : \", sample_chunks['ids'][0][i])\n    print(\"Paragraph : \", chunk)\n    print(\"Distance : \", sample_chunks['distances'][0][i])", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "=========\nParagraph index :  268\nParagraph :  Chris Carter (right-handed hitter)\n2.2 Oakland Athletics 2.3 Houston Astros 2.4 Milwaukee Brewers 2.5 New York Yankees 2.6 Return to the Oakland Athletics 3 Personal life 4 References 5 External links Early life and career ( edit ) Carter was born in Redwood City , California . At approximately age 7 or 8 , his family moved to Las Vegas . He attended Sierra Vista High School . In 2005 , Sierra Vista 's baseball team won the Nevada Interscholastic Activities Association Class 4A state championship . Professional career ( edit ) Draft and minors ( edit ) Carter was drafted by the Chicago White Sox in the 15th round of the 2005 Major League Baseball Draft . Carter began his professional career with the Bristol White Sox of the Rookie - level Appalachian League in 2005 . He hit 10 home runs and had 37 runs batted in ( RBIs ) . He played for two teams in the 2006 season . The teams included the Great Falls White Sox of the Rookie - level Pioneer League and the Kannapolis Intimidators of the Class A South Atlantic League . He had a combined total of 16 home runs and 63 RBIs . He played for Kannapolis in the 2007 season where he hit 25 home runs and had 93 RBIs . During the 2007 offseason , the White Sox traded Carter to the Arizona Diamondbacks for Carlos Quentin . Carter with the Athletics in 2012 Oakland Athletics ( edit ) Two weeks after\nDistance :  0.6766288876533508\n=========\nParagraph index :  272\nParagraph :  Chris Carter (right-handed hitter)\n, the New York Yankees signed Carter to a one - year contract , worth $3.5 million . Carter batted . 204 with eight home runs and 70 strikeouts before the Yankees designated him for assignment on June 24 . He was called back up by the Yankees on June 29 when his replacement at first base , Tyler Austin , landed on the disabled list . On July 4 , he was again designated for assignment , this time to make room for Ji - man Choi on the roster . He was released on July 10 . Return to the Oakland Athletics ( edit ) Carter signed a minor league contract with the Oakland Athletics on July 21 , 2017 , and was assigned to the Nashville Sounds of the PCL . Personal life ( edit ) Carter 's father , Vernon , played basketball for Rancho High School in North Las Vegas . Carter is a car enthusiast . He owns a Shelby Super Snake . References ( edit ) ^ Jump up to : `` Get to Know : Brewers first baseman Chris Carter '' . Retrieved February 17 , 2017 . ^ Jump up to : `` Powerful Carter always had a single focus '' . Retrieved February 17 , 2017 . Jump up ^ Merkin , Scott ( December 3 , 2007 ) . `` White Sox trade for outfielder Quentin '' . Chicago White Sox . Retrieved July 16 , 2008 . Jump\nDistance :  0.6858609318733215\n=========\nParagraph index :  269\nParagraph :  Chris Carter (right-handed hitter)\nhe was traded to Arizona , the Diamondbacks traded Carter , Carlos Gonz\u00e1lez , Brett Anderson , Aaron Cunningham , Greg Smith , and Dana Eveland to the Oakland Athletics for Dan Haren and Connor Robertson . He played for the Stockton Ports of the Class A-Advanced California League in the 2008 season where he hit 39 home runs and had 104 RBIs . Carter was named the California League Rookie of the Year for the 2008 season . In 2009 , Carter split time between the Midland RockHounds of the Class AA Texas League and the Sacramento River Cats of the Class AAA Pacific Coast League ( PCL ) , putting a . 329 batting average ( a 70 - point increase from 2008 ) , 28 homers and 115 RBIs combined . In 2008 and 2009 , Baseball America ranked Carter as one of the top 10 prospects in the Athletics ' organization . Also in 2008 and 2009 , Carter was the Oakland Athletics ' Minor League Player of Year . Carter was placed on the A 's 40 - man roster on November 20 , 2009 . In 2009 , he was named the This Year in Minor League Baseball Awards `` Overall Hitter of The Year '' . On August 9 , 2010 , Carter was promoted to Oakland and went 0 -- for -- 3 in his first game . On August 16 , Carter was demoted to Sacramento after starting his career 0\nDistance :  0.7151550054550171\n=========\nParagraph index :  271\nParagraph :  Chris Carter (right-handed hitter)\nslower for Carter , as he batted only . 153 throughout the entire month of April . Carter would turn his fortunes around after the All - Star break though , as finished with a . 227 batting average and career highs of 37 home runs and 88 RBI . On January 14 , 2015 , Carter and the Astros agreed to a one - year contract worth $4.175 million , avoiding arbitration . Carter had a disappointing 2015 season for the Astros ; Carter was the team 's starting first baseman , but hit only . 199 in 129 games . However , he still managed to hit 24 home runs , and then hit . 294 with a home run against the Kansas City Royals during the 2015 American League Division Series . At the conclusion of the 2015 season Carter was non tendered by the Astros and he became a free agent . Milwaukee Brewers ( edit ) On January 6 , 2016 , Carter signed a one - year , $2.5 million contract with the Milwaukee Brewers . He posted a . 321 on - base percentage and hit 41 home runs , leading the National League in 2016 . However , he had a . 222 batting average and led the league with 206 strikeouts . The Brewers did not tender Carter a contract for the 2017 , making him a free agent . New York Yankees ( edit ) On February 16 , 2017\nDistance :  0.808864414691925\n=========\nParagraph index :  275\nParagraph :  Chris Carter (right-handed hitter)\nExternal links ( edit ) Wikimedia Commons has media related to Chris Carter ( baseball player born 1986 ) . Career statistics and player information from MLB , or Baseball - Reference , or Fangraphs , or The Baseball Cube , or Baseball - Reference ( Minors ) ( hide ) National League season home run leaders 1876 : Hall 1877 : Pike 1878 : Hines 1879 : C. Jones 1880 : Stovey & O'Rourke 1881 : Brouthers 1882 : Wood 1883 : Ewing 1884 : Williamson 1885 : Dalrymple 1886 : Brouthers & Richardson 1887 : O'Brien 1888 : Ryan 1889 : Thompson 1890 : Burns , Tiernan & Wilmot 1891 : Tiernan & Stovey 1892 : Holliday 1893 : Delahanty 1894 : Duffy 1895 : Thompson 1896 : Joyce & Delahanty 1897 : Duffy 1898 : J. Collins 1899 : Freeman 1900 : Long 1901 : Crawford 1902 : Leach 1903 : Sheckard 1904 : Lumley 1905 : Odwell 1906 : Jordan 1907 : Brain 1908 : Jordan 1909 : Murray 1910 : Schulte & Beck 1911 : Schulte 1912 : Zimmerman 1913 : Cravath 1914 : Cravath 1915 : Cravath 1916 : C. Williams & Robertson 1917 : Cravath & Robertson 1918 : Cravath 1919 : Cravath 1920 : C. Williams 1921 : Kelly 1922 : Hornsby 1923 : C. Williams 1924 : Fournier 1925 : Hornsby 1926 : Wilson 1927 : C. Williams & Wilson 1928 : Wilson & Bottomley 1929 : Klein 1930 : Wilson\nDistance :  0.8511188626289368\n", "name": "stdout"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Feed the context and the questions to `watsonx.ai` model."}, {"metadata": {}, "cell_type": "markdown", "source": "Define instructions for the model.\n\n**Note:** Please start with finding better prompts using small subset of training records (under `train_data` variable). Make sure to not run an inference of all of `train_data`, as it'll take a long time to get the results. To get a sample from `train_data`, you can use e.g.`train_data.head(n=10)` to get first 10 records, or `train_data.sample(n=10)` to get random 10 records. Only once you have identified the best performing prompt, update this notebook to use the prompt and compute the metrics on the test data.\n\n**Action:** Please edit the below cell and add your own prompt here. In the below prompt, we have the instruction (first sentence) and one example included in the prompt. If you want to change the prompt or add your own examples or more examples, please change the below prompt accordingly."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "def make_prompt(context, question_text):\n    return (f\"Using the most relevant context, give the most accurate and precise answer. As a bot competing in a game of trivia, use reasoning to give the answers most likely to earn points for the highest score.\\n\"\n          + f\"{context}:\\n\\n\"\n          + f\"{question_text}\")\n\nprompt_texts = []\n\nfor relevant_context, question_text in zip(relevant_contexts, question_texts):\n    context = \"\\n\\n\\n\".join(relevant_context[\"documents\"][0])\n    prompt_text = make_prompt(context, question_text)\n    prompt_texts.append(prompt_text)", "execution_count": 20, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Inspect prompt for sample question."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "print(prompt_texts[0])", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Please answer the following.\nChris Carter (right-handed hitter)\n2.2 Oakland Athletics 2.3 Houston Astros 2.4 Milwaukee Brewers 2.5 New York Yankees 2.6 Return to the Oakland Athletics 3 Personal life 4 References 5 External links Early life and career ( edit ) Carter was born in Redwood City , California . At approximately age 7 or 8 , his family moved to Las Vegas . He attended Sierra Vista High School . In 2005 , Sierra Vista 's baseball team won the Nevada Interscholastic Activities Association Class 4A state championship . Professional career ( edit ) Draft and minors ( edit ) Carter was drafted by the Chicago White Sox in the 15th round of the 2005 Major League Baseball Draft . Carter began his professional career with the Bristol White Sox of the Rookie - level Appalachian League in 2005 . He hit 10 home runs and had 37 runs batted in ( RBIs ) . He played for two teams in the 2006 season . The teams included the Great Falls White Sox of the Rookie - level Pioneer League and the Kannapolis Intimidators of the Class A South Atlantic League . He had a combined total of 16 home runs and 63 RBIs . He played for Kannapolis in the 2007 season where he hit 25 home runs and had 93 RBIs . During the 2007 offseason , the White Sox traded Carter to the Arizona Diamondbacks for Carlos Quentin . Carter with the Athletics in 2012 Oakland Athletics ( edit ) Two weeks after\n\n\nChris Carter (right-handed hitter)\n, the New York Yankees signed Carter to a one - year contract , worth $3.5 million . Carter batted . 204 with eight home runs and 70 strikeouts before the Yankees designated him for assignment on June 24 . He was called back up by the Yankees on June 29 when his replacement at first base , Tyler Austin , landed on the disabled list . On July 4 , he was again designated for assignment , this time to make room for Ji - man Choi on the roster . He was released on July 10 . Return to the Oakland Athletics ( edit ) Carter signed a minor league contract with the Oakland Athletics on July 21 , 2017 , and was assigned to the Nashville Sounds of the PCL . Personal life ( edit ) Carter 's father , Vernon , played basketball for Rancho High School in North Las Vegas . Carter is a car enthusiast . He owns a Shelby Super Snake . References ( edit ) ^ Jump up to : `` Get to Know : Brewers first baseman Chris Carter '' . Retrieved February 17 , 2017 . ^ Jump up to : `` Powerful Carter always had a single focus '' . Retrieved February 17 , 2017 . Jump up ^ Merkin , Scott ( December 3 , 2007 ) . `` White Sox trade for outfielder Quentin '' . Chicago White Sox . Retrieved July 16 , 2008 . Jump\n\n\nChris Carter (right-handed hitter)\nhe was traded to Arizona , the Diamondbacks traded Carter , Carlos Gonz\u00e1lez , Brett Anderson , Aaron Cunningham , Greg Smith , and Dana Eveland to the Oakland Athletics for Dan Haren and Connor Robertson . He played for the Stockton Ports of the Class A-Advanced California League in the 2008 season where he hit 39 home runs and had 104 RBIs . Carter was named the California League Rookie of the Year for the 2008 season . In 2009 , Carter split time between the Midland RockHounds of the Class AA Texas League and the Sacramento River Cats of the Class AAA Pacific Coast League ( PCL ) , putting a . 329 batting average ( a 70 - point increase from 2008 ) , 28 homers and 115 RBIs combined . In 2008 and 2009 , Baseball America ranked Carter as one of the top 10 prospects in the Athletics ' organization . Also in 2008 and 2009 , Carter was the Oakland Athletics ' Minor League Player of Year . Carter was placed on the A 's 40 - man roster on November 20 , 2009 . In 2009 , he was named the This Year in Minor League Baseball Awards `` Overall Hitter of The Year '' . On August 9 , 2010 , Carter was promoted to Oakland and went 0 -- for -- 3 in his first game . On August 16 , Carter was demoted to Sacramento after starting his career 0\n\n\nChris Carter (right-handed hitter)\nslower for Carter , as he batted only . 153 throughout the entire month of April . Carter would turn his fortunes around after the All - Star break though , as finished with a . 227 batting average and career highs of 37 home runs and 88 RBI . On January 14 , 2015 , Carter and the Astros agreed to a one - year contract worth $4.175 million , avoiding arbitration . Carter had a disappointing 2015 season for the Astros ; Carter was the team 's starting first baseman , but hit only . 199 in 129 games . However , he still managed to hit 24 home runs , and then hit . 294 with a home run against the Kansas City Royals during the 2015 American League Division Series . At the conclusion of the 2015 season Carter was non tendered by the Astros and he became a free agent . Milwaukee Brewers ( edit ) On January 6 , 2016 , Carter signed a one - year , $2.5 million contract with the Milwaukee Brewers . He posted a . 321 on - base percentage and hit 41 home runs , leading the National League in 2016 . However , he had a . 222 batting average and led the league with 206 strikeouts . The Brewers did not tender Carter a contract for the 2017 , making him a free agent . New York Yankees ( edit ) On February 16 , 2017\n\n\nChris Carter (right-handed hitter)\nExternal links ( edit ) Wikimedia Commons has media related to Chris Carter ( baseball player born 1986 ) . Career statistics and player information from MLB , or Baseball - Reference , or Fangraphs , or The Baseball Cube , or Baseball - Reference ( Minors ) ( hide ) National League season home run leaders 1876 : Hall 1877 : Pike 1878 : Hines 1879 : C. Jones 1880 : Stovey & O'Rourke 1881 : Brouthers 1882 : Wood 1883 : Ewing 1884 : Williamson 1885 : Dalrymple 1886 : Brouthers & Richardson 1887 : O'Brien 1888 : Ryan 1889 : Thompson 1890 : Burns , Tiernan & Wilmot 1891 : Tiernan & Stovey 1892 : Holliday 1893 : Delahanty 1894 : Duffy 1895 : Thompson 1896 : Joyce & Delahanty 1897 : Duffy 1898 : J. Collins 1899 : Freeman 1900 : Long 1901 : Crawford 1902 : Leach 1903 : Sheckard 1904 : Lumley 1905 : Odwell 1906 : Jordan 1907 : Brain 1908 : Jordan 1909 : Murray 1910 : Schulte & Beck 1911 : Schulte 1912 : Zimmerman 1913 : Cravath 1914 : Cravath 1915 : Cravath 1916 : C. Williams & Robertson 1917 : Cravath & Robertson 1918 : Cravath 1919 : Cravath 1920 : C. Williams 1921 : Kelly 1922 : Hornsby 1923 : C. Williams 1924 : Fournier 1925 : Hornsby 1926 : Wilson 1927 : C. Williams & Wilson 1928 : Wilson & Bottomley 1929 : Klein 1930 : Wilson:\n\nwho did chris carter play for last year?\n", "name": "stdout"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the model parameters\nWe need to provide a set of model parameters that will influence the result:"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 50\n}", "execution_count": 22, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Initialize the `Model` class."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models import Model\n\nmodel = Model(\n    model_id=model_id,\n    params=parameters,\n    credentials=credentials,\n    project_id=project_id)", "execution_count": 23, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Generate a retrieval-augmented response"}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** Execution of this cell could take several minutes."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "results = []\n\nfor prompt_text in prompt_texts:\n    results.append(model.generate_text(prompt=prompt_text))", "execution_count": 24, "outputs": []}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "for idx, result in enumerate(results):\n    print(\"Question = \", test_data.iloc[idx]['question'])\n    print(\"Answer = \", result)\n    print(\"Expected Answer(s) (may not be appear with exact wording in the dataset) = \", test_data.iloc[idx]['answers'])\n    print(\"\\n\")", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "Question =  who did chris carter play for last year\nAnswer =  Milwaukee Brewers\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Milwaukee Brewers\n\n\nQuestion =  what is the latest version of safari on mac\nAnswer =  10.1. 2\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Safari 11\n\n\nQuestion =  when did bucharest become the capital of romania\nAnswer =  1862\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  1862\n\n\nQuestion =  who did jeffrey dean morgan play on supernatural\nAnswer =  John Winchester\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  John Eric Winchester\n\n\nQuestion =  who is the shortest man that ever lived\nAnswer =  Chandra Bahadur Dangi\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Chandra Bahadur Dangi\n\n\nQuestion =  how many seconds do you have to throw a grenade\nAnswer =  4\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  4 and 5 seconds\n\n\nQuestion =  who is known as a father of indian cricket\nAnswer =  Buchi Babu Naidu\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  M. Suryanarayan\n\n\nQuestion =  what is the name of the period in japanese history that began in 1868\nAnswer =  Meiji\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Meiji\n\n\nQuestion =  harold and kumar go to white castle where was it filmed\nAnswer =  Toronto\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Toronto , Ontario , Canada\n\n\nQuestion =  how many games have the capitals won in the playoffs\nAnswer =  446\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  two\n\n\nQuestion =  what is the best selling nintendo game of all time\nAnswer =  Super Mario Bros\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Wii Sports\n\n\nQuestion =  korean movie about a man on an island\nAnswer =  Jump up                        \nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Castaway on the Moon\n\n\nQuestion =  who plays lorelai in hannah montana the movie\nAnswer =  Melora Hardin\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Melora Hardin\n\n\nQuestion =  what season of greys anatomy was the plane crash\nAnswer =  seventh\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  the eighth season\n\n\nQuestion =  who made call of duty black ops 2\nAnswer =  Treyarch\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Treyarch\n\n\nQuestion =  when was the last solar eclipse seen in north america\nAnswer =  August 21, 2017\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  August 21 , 2017\n\n\nQuestion =  who is hosting the fifa world cup in 2022\nAnswer =  Qatar\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Qatar\n\n\nQuestion =  what is the record for wins in major league baseball\nAnswer =  116\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  116\n\n\nQuestion =  big boss 2 telugu set location in hyderabad\nAnswer =  Bigg Boss Telugu\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Annapurna Studios , Hyderabad\n\n\nQuestion =  who did the us support in the korean war\nAnswer =  South Korea\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  South Korea\n\n\nQuestion =  what is the caterpillar smoking in alice in wonderland\nAnswer =  hookah\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  a hookah\n\n\nQuestion =  how much aid does us give to other countries\nAnswer =  less than 1 percent of the US federal budget goes towards foreign aid\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  $43.10 billion\n\n\nQuestion =  who plays jake on two and a half\nAnswer =  Charlie Weber\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Angus Turner Jones\n\n\nQuestion =  what do you call a bundle of hay\nAnswer =  haystack\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  bales\n\n\nQuestion =  how many levels are there in science olympiad\nAnswer =  three\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  two\n\n\nQuestion =  where are the singers of florida georgia line from\nAnswer =  Florida Georgia Line\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Monroe , Georgia::Ormond Beach , Florida\n\n\nQuestion =  who sings lead on wouldnt it be nice\nAnswer =  Brian Wilson\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Brian Wilson::Mike Love\n\n\nQuestion =  who is the main character in shadow of mordor\nAnswer =  Talion\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Talion\n\n\nQuestion =  where are the next olympics to be held\nAnswer =  Pyeongchang\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Tokyo::Beijing\n\n\nQuestion =  when did game of thrones season 7 start\nAnswer =  July 16\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  July 16 , 2017\n\n\nQuestion =  when does hook show up in once upon a time\nAnswer =  season 2\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  The Crocodile\n\n\nQuestion =  when was the last time georgia tech won a national championship\nAnswer =  2017\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  1990\n\n\nQuestion =  who plays victor in days of our lives\nAnswer =  john aniston\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  John Anthony Aniston\n\n\nQuestion =  who won the last triple crown in baseball\nAnswer =  Miguel Cabrera\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Miguel Cabrera\n\n\nQuestion =  when does the football transfer window open in 2018\nAnswer =  on August 9\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  August 9\n\n\nQuestion =  what episode of mr young do adam and echo kiss\nAnswer =  Mr. Meteor\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  `` Mr. First Impression ''\n\n\nQuestion =  when is game of thrones season 7 episode 7 releasing\nAnswer =  Game of Thrones (season 7)\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  July 16 , 2017\n\n\nQuestion =  who is the architect of sheikh zayed mosque\nAnswer =  Yousef Abdelky\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Yousef Abdelky\n\n\nQuestion =  who is the director of iron man 3\nAnswer =  Shane Black\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Shane Black\n\n\nQuestion =  who is one of the first german composers that we know about\nAnswer =  Johann Jakob Froberger\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Adam von Fulda\n\n\nQuestion =  where do they move to in cheaper by the dozen\nAnswer =  Evanston\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Evanston , IL\n\n\nQuestion =  who played bass on and justice for all\nAnswer =  Jason Newsted\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Jason Newsted\n\n\nQuestion =  zen and the art of motorcycle maintenance bikes\nAnswer =  Honda CB77\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  CB77 Super Hawk\n\n\nQuestion =  when did texas become part of united states\nAnswer =  December 29 , 1845\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  1845\n\n\nQuestion =  the atlantoaxial joint is an example of what type of joint\nAnswer =  pivot\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  a pivot joint\n\n\nQuestion =  what is the fastest roller coaster in california\nAnswer =  Goliath\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Formula Rossa\n\n\nQuestion =  when did they start to build the great wall of china\nAnswer =  771 -- 476 BC\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  771 -- 476 BC\n\n\nQuestion =  what type of reaction is the rusting of iron\nAnswer =  electrochemical\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  electrochemical\n\n\nQuestion =  where do the flyers play their home games\nAnswer =  Wells Fargo Center\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Wells Fargo Center\n\n\nQuestion =  where does something old something new something borrowed something blue come from\nAnswer =  Lancashire\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Lancashire\n\n\n", "name": "stdout"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"score\"></a>\n## Calculate rougeL metric"}, {"metadata": {}, "cell_type": "markdown", "source": "In this sample notebook `rouge_score` module was used for rougeL calculation."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "#### Rouge Metric"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "**Note:** The Rouge (Recall-Oriented Understudy for Gisting Evaluation) metric is a set of evaluation measures used in natural language processing (NLP) and specifically in text summarization and machine translation tasks. The Rouge metrics are designed to assess the quality of generated summaries or translations by comparing them to one or more reference texts.\n\nThe main idea behind Rouge is to measure the overlap between the generated summary (or translation) and the reference text(s) in terms of n-grams or longest common subsequences. By calculating recall, precision, and F1 scores based on these overlapping units, Rouge provides a quantitative assessment of the summary's content overlap with the reference(s).\n\nRouge-1 focuses on individual word overlap, Rouge-2 considers pairs of consecutive words, and Rouge-L takes into account the ordering of words and phrases. These metrics provide different perspectives on the similarity between two texts and can be used to evaluate different aspects of summarization or text generation models."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from rouge_score import rouge_scorer\nfrom collections import defaultdict\nimport numpy as np\n\ndef get_rouge_score(predictions, references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'])\n    aggregate_score = defaultdict(list)\n\n    for result, ref in zip(predictions, references):\n        for key, val in scorer.score(result, ref).items():\n            aggregate_score[key].append(val.fmeasure)\n\n    scores = {}\n    for key in aggregate_score:\n        scores[key] = np.mean(aggregate_score[key])\n    \n    return scores", "execution_count": 26, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(get_rouge_score(results, test_data.answers))", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "{'rouge1': 0.5479999999999999, 'rouge2': 0.25666666666666665, 'rougeL': 0.5429999999999999, 'rougeLsum': 0.5429999999999999}\n", "name": "stdout"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "---"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Copyright \u00a9 2023 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}